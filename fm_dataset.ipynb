{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import zarr \n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainval_root = Path(\"/Data_large/marine/PythonProjects/OtherProjects/lpl-PyNas/data/Phisat2Simulation/TrainVal\")\n",
    "trainval_imgs = trainval_root / \"numpy_images\"\n",
    "trainval_masks = trainval_root / \"numpy_masks\"\n",
    "# list all imgs and masks\n",
    "trainval_imgs = sorted(trainval_imgs.glob(\"*.npy\"))\n",
    "# find the corresponding mask for each img\n",
    "trainval_masks = [trainval_masks / img.name.replace('image','mask') for img in trainval_imgs]\n",
    "###################################\n",
    "test_root = Path(\"/Data_large/marine/PythonProjects/OtherProjects/lpl-PyNas/data/Phisat2Simulation/Test\")\n",
    "test_imgs = test_root / \"numpy_images\"\n",
    "test_masks = test_root / \"numpy_masks\"\n",
    "# list all imgs and masks\n",
    "test_imgs = sorted(test_imgs.glob(\"*.npy\"))\n",
    "test_masks = [test_masks / img.name.replace('image','mask') for img in test_imgs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sample(root, \n",
    "               dataset_set, \n",
    "               idx, \n",
    "               task, \n",
    "               img, \n",
    "               label,\n",
    "               metadata=None,\n",
    "               overwrite=False):\n",
    "    \"\"\"\n",
    "    Adds a sample to the Zarr dataset.\n",
    "\n",
    "    Args:\n",
    "        root (zarr.Group): Root of the Zarr dataset.\n",
    "        dataset_set (str): One of {\"trainval\", \"test\"} (case-insensitive).\n",
    "        idx (int): Sample index.\n",
    "        task (str): One of {\"classification\", \"segmentation\", \"regression\", \"compression\"} (case-insensitive).\n",
    "        img (np.ndarray): Image array, shape (C, H, W).\n",
    "        label (np.ndarray): Label array; 1D for classification, 3D for other tasks.\n",
    "        metadata (dict, optional): Metadata to attach to the sample group.\n",
    "        overwrite (bool): If True, overwrites existing sample with same index.\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: If types or shapes are invalid.\n",
    "        ValueError: If task or dataset_set is not recognized.\n",
    "    \"\"\"\n",
    "    dataset_set = dataset_set.lower()\n",
    "    task = task.lower()\n",
    "\n",
    "    assert isinstance(idx, int), \"idx must be an integer\"\n",
    "    assert isinstance(img, np.ndarray) and img.ndim == 3, \"img must be a 3D numpy array\"\n",
    "    assert isinstance(label, np.ndarray), \"label must be a numpy array\"\n",
    "    assert dataset_set in {\"trainval\", \"test\"}, f\"Unknown dataset_set: {dataset_set}\"\n",
    "    assert task in {\"classification\", \"segmentation\", \"regression\", \"compression\"}, f\"Unknown task: {task}\"\n",
    "\n",
    "    if task == \"classification\":\n",
    "        assert label.ndim == 1, \"label must be 1D for classification\"\n",
    "    else:\n",
    "        assert label.ndim == 3, \"label must be 3D for non-classification tasks\"\n",
    "        assert img.shape[1:] == label.shape[1:], \"img and label must have same spatial dimensions\"\n",
    "        if task == \"compression\":\n",
    "            assert img.shape[0] == label.shape[0], \"img and label must have same number of channels for compression\"\n",
    "\n",
    "    # Ensure the dataset_set group exists\n",
    "    if dataset_set not in root:\n",
    "        root.create_group(dataset_set)\n",
    "        \n",
    "    dataset_group = root[dataset_set]\n",
    "    sample_id = f\"{idx:07d}\"\n",
    "\n",
    "    if sample_id in dataset_group:\n",
    "        if overwrite:\n",
    "            del dataset_group[sample_id]\n",
    "        else:\n",
    "            raise FileExistsError(f\"Sample '{dataset_set}/{sample_id}' already exists. Use overwrite=True to replace.\")\n",
    "\n",
    "    g = dataset_group.create_group(sample_id)\n",
    "    g.create_dataset(\"img\", data=img.astype(np.float32))\n",
    "    g.create_dataset(\"label\", data=label.astype(np.float32))\n",
    "\n",
    "    # ---- Metadata ----\n",
    "    meta = metadata or {}\n",
    "\n",
    "    # Task attribute\n",
    "    g.attrs[\"task\"] = task\n",
    "\n",
    "    # Sensor-related\n",
    "    g.attrs.update({\n",
    "        \"sensor\": meta.get(\"sensor\", \"S2A\"),\n",
    "        \"sensor_resolution\": meta.get(\"sensor_resolution\", 10),\n",
    "        \"sensor_orbit\": meta.get(\"sensor_orbit\", \"ASCENDING\"),\n",
    "        \"spectral_bands_ordered\": meta.get(\"spectral_bands_ordered\", \"B2-B3-B4-B4\"),\n",
    "        \"sensor_orbit_number\": meta.get(\"sensor_orbit_number\", 0),\n",
    "        \"datatake\": meta.get(\"datatake\", \"00-00-0000 00:00:00\")\n",
    "    })\n",
    "\n",
    "    # Geolocation\n",
    "    geo = meta.get(\"geolocation\", {})\n",
    "    g.attrs[\"geolocation\"] = {\n",
    "        \"UL\": geo.get(\"UL\", [np.nan, np.nan]),\n",
    "        \"UR\": geo.get(\"UR\", [np.nan, np.nan]),\n",
    "        \"LL\": geo.get(\"LL\", [np.nan, np.nan]),\n",
    "        \"LR\": geo.get(\"LR\", [np.nan, np.nan])\n",
    "    }\n",
    "\n",
    "    # Ancillary\n",
    "    anc = meta.get(\"ancillary\", {})\n",
    "    g.attrs.update({\n",
    "        \"cloud_cover\": anc.get(\"cloud_cover\", np.nan),\n",
    "        \"sun_azimuth\": anc.get(\"sun_azimuth\", np.nan),\n",
    "        \"sun_elevation\": anc.get(\"sun_elevation\", np.nan),\n",
    "        \"view_azimuth\": anc.get(\"view_azimuth\", np.nan),\n",
    "        \"view_elevation\": anc.get(\"view_elevation\", np.nan)\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dataset Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation:\n",
    "1. First ensures the dataset group (\"trainval\" or \"test\") exists\n",
    "2. Gets a reference to that group\n",
    "3. Works with sample IDs inside that group\n",
    "4. Maintains the same functionality while being more explicit about the hierarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) TrainVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = zarr.open(\"burned_area_dataset.zarr\", mode=\"w\")\n",
    "\n",
    "dataset_set = \"trainval\"\n",
    "task = \"segmentation\"\n",
    "\n",
    "for idx, (img, mask) in enumerate(zip(trainval_imgs, trainval_masks)):\n",
    "    img = np.load(img)\n",
    "    mask = np.load(mask)\n",
    "    # H, W, C -> C, H, W\n",
    "    img = np.moveaxis(img, -1, 0)\n",
    "    mask = np.moveaxis(mask, -1, 0)\n",
    "    \n",
    "    print(f\"Image shape: {img.shape}, Mask shape: {mask.shape}\")\n",
    "    # Assuming img and mask are already loaded as numpy arrays\n",
    "    add_sample(root, \n",
    "               dataset_set, \n",
    "               idx, \n",
    "               task, \n",
    "               img, \n",
    "               label=mask,\n",
    "               metadata=None,\n",
    "               overwrite=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_set = \"test\" # or trainval\n",
    "task = \"segmentation\"\n",
    "\n",
    "for idx, (img, mask) in enumerate(zip(test_imgs, test_masks)):\n",
    "    img = np.load(img)\n",
    "    mask = np.load(mask)\n",
    "    # H, W, C -> C, H, W\n",
    "    img = np.moveaxis(img, -1, 0)\n",
    "    mask = np.moveaxis(mask, -1, 0)\n",
    "    \n",
    "    print(f\"Image shape: {img.shape}, Mask shape: {mask.shape}\")\n",
    "    # Assuming img and mask are already loaded as numpy arrays\n",
    "    add_sample(root, \n",
    "               dataset_set, \n",
    "               idx, \n",
    "               task, \n",
    "               img, \n",
    "               label=mask,\n",
    "               metadata=None,\n",
    "               overwrite=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B) Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 487/487 [00:30<00:00, 16.00it/s]\n"
     ]
    }
   ],
   "source": [
    "from data_loader import get_zarr_dataloader, NormalizeChannels\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Example usage\n",
    "zarr_path = \"burned_area_dataset.zarr\"\n",
    "\n",
    "dataset_set = \"trainval\"\n",
    "# Create DataLoader\n",
    "dataloader = get_zarr_dataloader(\n",
    "    zarr_path=zarr_path,\n",
    "    dataset_set=dataset_set,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    transform=NormalizeChannels(min_max=True),\n",
    "    task_filter=\"segmentation\",\n",
    "    metadata_keys=[\"sensor\", \"timestamp\"],\n",
    ")\n",
    "\n",
    "# Iterate through batches\n",
    "for idx, batch in enumerate(tqdm(dataloader, desc=\"Processing Batches\")):\n",
    "    # Access data based on tasks in the batch\n",
    "    for task in batch['tasks']:\n",
    "        images = batch[f'{task}_img']\n",
    "        labels = batch[f'{task}_label']\n",
    "        # Forward pass, compute loss, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pynas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
